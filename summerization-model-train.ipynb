{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Ref\n# https://huggingface.co/docs/evaluate/main/en/transformers_integrations\n# https://www.kaggle.com/code/fadyelkbeer/mt5-multilingual-xlsum\n# https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_&_Biases.ipynb\n# https://docs.wandb.ai/guides/integrations/huggingface","metadata":{"id":"33UfnSX2l0Aw","execution":{"iopub.status.busy":"2023-10-12T06:15:43.277793Z","iopub.execute_input":"2023-10-12T06:15:43.278594Z","iopub.status.idle":"2023-10-12T06:15:43.287949Z","shell.execute_reply.started":"2023-10-12T06:15:43.278554Z","shell.execute_reply":"2023-10-12T06:15:43.286819Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate rouge_score wandb transformers[torch] sentencepiece -q\n!pip install accelerate -U -q\n!pip install datasets -U -q","metadata":{"id":"WZaJCBzkl0Ax","execution":{"iopub.status.busy":"2023-10-12T06:15:43.293453Z","iopub.execute_input":"2023-10-12T06:15:43.295226Z","iopub.status.idle":"2023-10-12T06:16:17.554552Z","shell.execute_reply.started":"2023-10-12T06:15:43.295177Z","shell.execute_reply":"2023-10-12T06:16:17.553270Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ns3fs 2023.9.0 requires fsspec==2023.9.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil","metadata":{"id":"BMRVww_P0oli","execution":{"iopub.status.busy":"2023-10-12T06:16:17.556313Z","iopub.execute_input":"2023-10-12T06:16:17.556732Z","iopub.status.idle":"2023-10-12T06:16:17.562190Z","shell.execute_reply.started":"2023-10-12T06:16:17.556694Z","shell.execute_reply":"2023-10-12T06:16:17.561214Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"continute_train = True # @param {type:\"boolean\"}","metadata":{"id":"MqV2-TYM33d7","execution":{"iopub.status.busy":"2023-10-12T06:16:17.563353Z","iopub.execute_input":"2023-10-12T06:16:17.563690Z","iopub.status.idle":"2023-10-12T06:16:17.577090Z","shell.execute_reply.started":"2023-10-12T06:16:17.563659Z","shell.execute_reply":"2023-10-12T06:16:17.576007Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Wandb Setup","metadata":{"id":"1WDPdk5jl0Ay"}},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"633eeb120e6c05fe397c3e72cd4fda73233e2b23\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0fAdk4fl0Ay","outputId":"2a49eb73-8398-41db-9a7d-ad197243ada9","execution":{"iopub.status.busy":"2023-10-12T06:16:17.580385Z","iopub.execute_input":"2023-10-12T06:16:17.580733Z","iopub.status.idle":"2023-10-12T06:16:20.895924Z","shell.execute_reply.started":"2023-10-12T06:16:17.580701Z","shell.execute_reply":"2023-10-12T06:16:20.894846Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# @title ##Initialize wandb\nproject_name = \"text-summarization-model\"\n\nrun = wandb.init(project=project_name, job_type=\"train\")#, name=run_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"FwEpGnvll0A0","outputId":"e4ed3173-2d4b-4f9c-e7b0-1e738ac94b9d","execution":{"iopub.status.busy":"2023-10-12T06:16:20.897445Z","iopub.execute_input":"2023-10-12T06:16:20.898197Z","iopub.status.idle":"2023-10-12T06:16:51.672864Z","shell.execute_reply.started":"2023-10-12T06:16:20.898163Z","shell.execute_reply":"2023-10-12T06:16:51.671780Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdylanonwic\u001b[0m (\u001b[33mdylanon\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231012_061621-ymiv1hbe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dylanon/text-summarization-model/runs/ymiv1hbe' target=\"_blank\">autumn-energy-53</a></strong> to <a href='https://wandb.ai/dylanon/text-summarization-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dylanon/text-summarization-model' target=\"_blank\">https://wandb.ai/dylanon/text-summarization-model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dylanon/text-summarization-model/runs/ymiv1hbe' target=\"_blank\">https://wandb.ai/dylanon/text-summarization-model/runs/ymiv1hbe</a>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Get tokenizer","metadata":{"id":"v1GS15Cvl0Ay"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\", legacy=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxfOs4oyl0Az","outputId":"97c80ffb-d2b9-4e0d-8a04-796ec2e47ece","execution":{"iopub.status.busy":"2023-10-12T06:16:51.674226Z","iopub.execute_input":"2023-10-12T06:16:51.674555Z","iopub.status.idle":"2023-10-12T06:16:57.212158Z","shell.execute_reply.started":"2023-10-12T06:16:51.674522Z","shell.execute_reply":"2023-10-12T06:16:57.211063Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5cb1b87717745019d27c8b77db5f315"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ed952827a14bbea3bac3794027e979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6059c6c80247469092ee56649ad9a816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22cf557865b848168a984d3e3d26c00e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup evaluation","metadata":{"id":"5XpJGQcrl0Az"}},{"cell_type":"code","source":"import nltk\nimport evaluate\nnltk.download(\"punkt\", quiet=True)\nmetric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # rougeLSum expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return result","metadata":{"id":"FYOAT5yYl0Az","execution":{"iopub.status.busy":"2023-10-12T06:16:57.217184Z","iopub.execute_input":"2023-10-12T06:16:57.220358Z","iopub.status.idle":"2023-10-12T06:17:13.346685Z","shell.execute_reply.started":"2023-10-12T06:16:57.220318Z","shell.execute_reply":"2023-10-12T06:17:13.345558Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95d67a197fb140c9bb2fbd97ca094801"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Get model","metadata":{"id":"5AgLWixrl0Az"}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import DataCollatorForSeq2Seq\nfrom datasets import load_dataset\nimport datasets\nimport numpy as np","metadata":{"id":"rL_yWpW7l0Az","execution":{"iopub.status.busy":"2023-10-12T06:17:13.348302Z","iopub.execute_input":"2023-10-12T06:17:13.349346Z","iopub.status.idle":"2023-10-12T06:17:13.384804Z","shell.execute_reply.started":"2023-10-12T06:17:13.349304Z","shell.execute_reply":"2023-10-12T06:17:13.383583Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_dir = \"./imported_model\"\n\nif continute_train:\n\n    artifact = run.use_artifact('dylanon/text-summarization-model/summarization-model:latest', type='model')\n\n    if not os.path.exists(model_dir):\n        artifact_dir = artifact.download(root=model_dir)\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nelse:\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\", cache_dir=model_dir)","metadata":{"id":"2Tm9_2j42P0n","execution":{"iopub.status.busy":"2023-10-12T06:17:13.386402Z","iopub.execute_input":"2023-10-12T06:17:13.387014Z","iopub.status.idle":"2023-10-12T06:17:52.200088Z","shell.execute_reply.started":"2023-10-12T06:17:13.386980Z","shell.execute_reply":"2023-10-12T06:17:52.198903Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact summarization-model:latest, 2241.47MB. 8 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \nDone. 0:0:24.1\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"id":"aCfoLiFFl0Az","execution":{"iopub.status.busy":"2023-10-12T06:24:47.568545Z","iopub.status.idle":"2023-10-12T06:24:47.569316Z","shell.execute_reply.started":"2023-10-12T06:24:47.569088Z","shell.execute_reply":"2023-10-12T06:24:47.569111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get datasets","metadata":{}},{"cell_type":"code","source":"# load dataset-artifact\nartifact_dir = './imported_datasets'\nuse_artifact = \"dylanon/text-summarization-model/tokenized-dataset:latest\"\n\nif not os.path.exists(artifact_dir):\n    artifact = run.use_artifact(use_artifact, type='dataset')\n    artifact_dir = artifact.download(root=artifact_dir)\n\ntokenized_dataset = datasets.load_from_disk(artifact_dir)\ntokenized_dataset = datasets.Dataset.from_dict(tokenized_dataset[:100_000])\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=False, seed=1150)","metadata":{"id":"R2cYwl-ltldQ","execution":{"iopub.status.busy":"2023-10-12T06:24:47.570777Z","iopub.status.idle":"2023-10-12T06:24:47.571509Z","shell.execute_reply.started":"2023-10-12T06:24:47.571283Z","shell.execute_reply":"2023-10-12T06:24:47.571305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-12T06:25:00.260822Z","iopub.execute_input":"2023-10-12T06:25:00.261190Z","iopub.status.idle":"2023-10-12T06:25:00.271930Z","shell.execute_reply.started":"2023-10-12T06:25:00.261163Z","shell.execute_reply":"2023-10-12T06:25:00.270810Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 36000\n    })\n    test: Dataset({\n        features: ['id', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 4000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train config","metadata":{"id":"guxDFuQvuXXr"}},{"cell_type":"code","source":"wandb.config = {\n    \"evaluation_strategy\":\"steps\",\n    \"auto_find_batch_size\" : True,\n#   \"per_device_train_batch_size\":32,\n#   \"per_device_eval_batch_size\":8,\n    \n    \"learning_rate\":4e-4, # default 5e-4\n    \"warmup_steps\":10,\n    \"weight_decay\":0.01, # default 0.01\n    \"label_smoothing_factor\":0.1,\n    \"gradient_accumulation_steps\":32,  #16\n    \"eval_accumulation_steps\":64,\n    \"gradient_checkpointing\":True,\n    \"optim\":\"adamw_hf\",\n    \"lr_scheduler_type\":\"cosine\",\n    \n    \n    \"num_train_epochs\":2,\n#   \"max_steps\" :10000,\n    \"save_steps\":0,\n    \"eval_steps\":100,\n    \"logging_steps\":5,\n    \"save_total_limit\":0,\n    \"load_best_model_at_end\":False,\n    \"fp16\":True, # this can use with cuda only\n    \"predict_with_generate\":True,\n}","metadata":{"id":"x-dYybbqrYHo","execution":{"iopub.status.busy":"2023-10-12T06:46:47.048548Z","iopub.execute_input":"2023-10-12T06:46:47.048951Z","iopub.status.idle":"2023-10-12T06:46:47.057811Z","shell.execute_reply.started":"2023-10-12T06:46:47.048922Z","shell.execute_reply":"2023-10-12T06:46:47.056028Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# train argment\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    report_to=\"wandb\",\n    **wandb.config\n)\n\ntrainer = Seq2SeqTrainer(\n    model = model,\n    args = training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"id":"uTD1ylhBqCCQ","execution":{"iopub.status.busy":"2023-10-12T06:46:47.062529Z","iopub.execute_input":"2023-10-12T06:46:47.062872Z","iopub.status.idle":"2023-10-12T06:46:47.085840Z","shell.execute_reply.started":"2023-10-12T06:46:47.062848Z","shell.execute_reply":"2023-10-12T06:46:47.084800Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{}},{"cell_type":"code","source":"# @title ##Train\n# train\nif continute_train: trainer.train(resume_from_checkpoint = './imported_model')\nelse: trainer.train()\n\n\ndirectory_path = \"./exported_model\"\n\n# Check if the directory exists\nif os.path.exists(directory_path):\n    try:\n        # Delete the directory and its contents\n        shutil.rmtree(directory_path)\n        print(f\"Directory '{directory_path}' deleted successfully.\")\n    except Exception as e:\n        print(f\"Error deleting '{directory_path}': {str(e)}\")\n    \n\n# export model to file\ntrainer.save_model(directory_path)","metadata":{"id":"lDE6jhP5qEGj","colab":{"base_uri":"https://localhost:8080/","height":545},"outputId":"2f0ef495-0ba6-4f7b-8479-614e7c334820","execution":{"iopub.status.busy":"2023-10-12T06:46:47.091268Z","iopub.execute_input":"2023-10-12T06:46:47.092300Z","iopub.status.idle":"2023-10-12T10:21:37.247732Z","shell.execute_reply.started":"2023-10-12T06:46:47.092267Z","shell.execute_reply":"2023-10-12T10:21:37.246477Z"},"trusted":true},"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [280/280 3:34:04, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>32365952.000000</td>\n      <td>2.316163</td>\n      <td>0.234592</td>\n      <td>0.059011</td>\n      <td>0.203165</td>\n      <td>0.203297</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>57960326.400000</td>\n      <td>2.316163</td>\n      <td>0.234592</td>\n      <td>0.059011</td>\n      <td>0.203165</td>\n      <td>0.203297</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# @title ##Evaluation\nevaluation_results = trainer.evaluate()#eval_dataset=dataset['test'])\nevaluation_results","metadata":{"id":"cEdo-4RXwSO5","execution":{"iopub.status.busy":"2023-10-12T10:21:37.258403Z","iopub.execute_input":"2023-10-12T10:21:37.258864Z","iopub.status.idle":"2023-10-12T10:36:25.119306Z","shell.execute_reply.started":"2023-10-12T10:21:37.258825Z","shell.execute_reply":"2023-10-12T10:36:25.117115Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='286' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [286/500 14:42 < 11:02, 0.32 it/s]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title ##Evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#eval_dataset=dataset['test'])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m evaluation_results\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:159\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    155\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m gen_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2968\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2965\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2967\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2968\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2972\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2978\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3157\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3156\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3157\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3158\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3159\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:282\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    280\u001b[0m ):\n\u001b[1;32m    281\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 282\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1681\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1674\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1675\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1676\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1677\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1678\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1679\u001b[0m     )\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:3036\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3031\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   3032\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3033\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3034\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(next_token_scores)\n\u001b[1;32m   3039\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:97\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:756\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    754\u001b[0m num_batch_hypotheses \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    755\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 756\u001b[0m banned_batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43m_calc_banned_ngram_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batch_hypotheses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, banned_tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(banned_batch_tokens):\n\u001b[1;32m    758\u001b[0m     scores[i, banned_tokens] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:698\u001b[0m, in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m ngram_size:\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[0;32m--> 698\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m \u001b[43m_get_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hypos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m banned_tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    700\u001b[0m     _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hypo_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)\n\u001b[1;32m    702\u001b[0m ]\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m banned_tokens\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/logits_process.py:659\u001b[0m, in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    657\u001b[0m generated_ngrams \u001b[38;5;241m=\u001b[39m [{} \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos)]\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_hypos):\n\u001b[0;32m--> 659\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mprev_input_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     generated_ngram \u001b[38;5;241m=\u001b[39m generated_ngrams[idx]\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# Loop through each n-gram of size ngram_size in the list of tokens (gen_tokens)\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# import gc\n# from torch import cuda\n# cuda.empty_cache()\n# gc.collect()","metadata":{"id":"rpdiJtntWJ41","execution":{"iopub.status.busy":"2023-10-12T10:36:25.120578Z","iopub.status.idle":"2023-10-12T10:36:25.121311Z","shell.execute_reply.started":"2023-10-12T10:36:25.121049Z","shell.execute_reply":"2023-10-12T10:36:25.121076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model to wandb","metadata":{}},{"cell_type":"code","source":"# @title ##Upload model to wandb\nart = wandb.Artifact(f\"summarization-model\", type=\"model\")\n\nfor dir in os.listdir(\"./exported_model\"):\n    art.add_file(os.path.join( \"./exported_model\" , dir))\n\n# wandb.log(evaluation_results)\nwandb.log_artifact(art)","metadata":{"id":"VhwLkLpcl0A0","execution":{"iopub.status.busy":"2023-10-12T10:36:52.201390Z","iopub.execute_input":"2023-10-12T10:36:52.202315Z","iopub.status.idle":"2023-10-12T10:37:07.100311Z","shell.execute_reply.started":"2023-10-12T10:36:52.202269Z","shell.execute_reply":"2023-10-12T10:37:07.099278Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"<Artifact summarization-model>"},"metadata":{}}]},{"cell_type":"code","source":"# from time import time\n# from google.colab import runtime\n# time.sleep(300)\n# runtime.unassign()","metadata":{"id":"t2rlxnISF9Op","execution":{"iopub.status.busy":"2023-10-12T10:36:25.124785Z","iopub.status.idle":"2023-10-12T10:36:25.125498Z","shell.execute_reply.started":"2023-10-12T10:36:25.125250Z","shell.execute_reply":"2023-10-12T10:36:25.125277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}