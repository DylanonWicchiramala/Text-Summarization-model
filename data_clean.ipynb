{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa    \n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import re\n",
    "from pythainlp import util\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.abspath(\"datasets\")\n",
    "clean_dataset_path = os.path.abspath(\"datasets/clean\")\n",
    "\n",
    "dataset_paths = os.listdir(dataset_path)\n",
    "dataset_paths.remove('clean')\n",
    "dataset_paths = [os.path.join(dataset_path, dir) for dir in dataset_paths]\n",
    "dataset_names = [os.path.basename(path).split('.')[0] for path in dataset_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets from path\n",
    "datasets = dict(zip(dataset_names,[pq.read_table(path) for path in dataset_paths]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text: string\n",
       "sum: string"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pa.table({\n",
    "    \"text\":['UNK hlello`` \\'\\' world','\\n\\n\\n-lrb- olo  \\n\\n(SHRF) ain\\'t ‘yolo”\\n'],\n",
    "    \"sum\":['UNKhelUNKlo','olo']\n",
    "}, schema=pa.schema([\n",
    "    ('text', pa.string()),\n",
    "    ('sum', pa.string())\n",
    "]))\n",
    "t.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_processing(function, iterable, num_batch=65536):\n",
    "    def batch(iterable, n=num_batch):\n",
    "        l = len(iterable)\n",
    "        for ndx in range(0, l, n):\n",
    "            yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "    if num_batch == 0:\n",
    "        return list(map(function, iterable))\n",
    "    else:\n",
    "        out_bat = []\n",
    "        for bat in batch(iterable, num_batch):\n",
    "            out_bat = out_bat + list(map(function, bat))\n",
    "        return out_bat\n",
    "    \n",
    "\n",
    "def pa_map(func):\n",
    "    batch_size = 32768\n",
    "    def wrapper(table):\n",
    "        dict = table.to_pydict()\n",
    "        schema = table.schema\n",
    "        n_sample = table.num_rows\n",
    "        del table\n",
    "        # dict['text'] = list(map(func, dict['text'])) \n",
    "        # dict['sum'] = list(map(func, dict['sum']))\n",
    "        dict['text'] = batch_processing(func, dict['text'], batch_size)\n",
    "        dict['sum'] = batch_processing(func, dict['sum'], batch_size)\n",
    "        table = pa.table(dict, schema=schema)\n",
    "        del dict\n",
    "        return table\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize spatial token in each datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pa_map\n",
    "def remove_spcial_tokens_gigaword(input):\n",
    "    input = str(input)\n",
    "    input = input.replace(\"-lrb-\", \"\")\n",
    "    input = input.replace(\"UNK\", \"\")\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pa_map\n",
    "def remove_spcial_tokens_thaisum(input):\n",
    "    input = str(input)\n",
    "    input = input.replace(\"(SHRF)\",\"\")\n",
    "    input = input.replace(\"(SSA/SSPP)\",\"\")\n",
    "    input = input.replace(\"(SSPP)\",\"\")\n",
    "    input = input.replace(\"(SSA)\",\"\")\n",
    "    input = input.replace(u\"\\xa0\",\" \")\n",
    "    input = re.sub(r\"\\[.*\\]\",\"\", input)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pa_map\n",
    "def normalize_space(input):\n",
    "    input = str(input)\n",
    "    input = input.strip()\n",
    "    input = re.sub(\"\\n+\", \"\\n\", input)\n",
    "    input = re.sub(\" +\", \" \", input)\n",
    "    input = re.sub(\"\\t+\", \"\\t\", input)\n",
    "    return input\n",
    "\n",
    "\n",
    "@pa_map\n",
    "def normalize_quotation(input):\n",
    "    input = str(input)\n",
    "    input = re.sub(\"“\", \"\\\"\", input)\n",
    "    input = re.sub(\"”\", \"\\\"\", input)\n",
    "    input = re.sub(\"‘\", \"'\", input)\n",
    "    input = re.sub(\"’\", \"'\", input)\n",
    "    input = re.sub(\"‘\", \"'\", input)\n",
    "    input = re.sub(\"’\", \"'\", input)\n",
    "    return input    \n",
    "\n",
    "\n",
    "@pa_map\n",
    "def expand_contractions(input):\n",
    "    \n",
    "    input = str(input)\n",
    "    \n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    \n",
    "    contractions_dict = { \"ain't\": \"are not\", \"'s\":\" is\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"‘cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what've\": \"what have\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who've\": \"who have\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "    return contractions_re.sub(replace, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize thai text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pa_map\n",
    "def vowel_normalize(input: str):\n",
    "    '''\n",
    "    input: list of string of string \n",
    "    output: list of string or string\n",
    "    normalize เ เ, ํ า, double of same vowel(such as โโ, แแ,  ูู, ุุ ,่่) , and wrong order of vowel and tonal(such as ก่ิง)\n",
    "    '''\n",
    "    return util.normalize(str(input))\n",
    "\n",
    "\n",
    "@pa_map\n",
    "def thai_digit_to_arabic_digit(input: str):\n",
    "    '''\n",
    "    input: list of string of string \n",
    "    output: list of string or string\n",
    "    change thai digit(๑๒๓๔) to string of arabic digit\n",
    "    \n",
    "    example:\n",
    "        thai_digit_to_arabic_digit('๑๒๓๔กขคabc') -> '1234กขคabc'\n",
    "    '''\n",
    "    return util.thai_digit_to_arabic_digit(str(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "process normalize datasets gigaword.: 100%|██████████| 5/5 [06:28<00:00, 77.75s/it] \n",
      "process normalize datasets thaisum.: 100%|██████████| 5/5 [02:19<00:00, 28.00s/it]\n",
      "process normalize datasets tldr-17.: 100%|██████████| 5/5 [35:30<00:00, 426.18s/it]\n",
      "process normalize datasets wikihow.: 100%|██████████| 5/5 [02:08<00:00, 25.70s/it]\n",
      "process normalize datasets thaisum.: 100%|██████████| 2/2 [00:52<00:00, 26.17s/it]\n"
     ]
    }
   ],
   "source": [
    "func_q = [\n",
    "    remove_spcial_tokens_gigaword,\n",
    "    remove_spcial_tokens_thaisum,\n",
    "    normalize_space,\n",
    "    normalize_quotation,\n",
    "    expand_contractions\n",
    "]\n",
    "\n",
    "th_func_q = [\n",
    "    vowel_normalize,\n",
    "    thai_digit_to_arabic_digit\n",
    "]\n",
    "\n",
    "thai_dataset = [\n",
    "    'thaisum'\n",
    "]\n",
    "  \n",
    "  \n",
    "for key in datasets.keys():\n",
    "    for i in tqdm(range(len(func_q)), desc=f'process normalize datasets {key}.'):\n",
    "        datasets[key] = func_q[i](datasets[key])\n",
    "\n",
    "\n",
    "for key in thai_dataset:\n",
    "    for i in tqdm(range(len(th_func_q)), desc=f'process normalize datasets {key}.'):\n",
    "        datasets[key] = func_q[i](datasets[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_pro(func):\n",
    "#     def wrap(input):\n",
    "#         return{\n",
    "#             \"text\": func(input['text']),\n",
    "#             \"sum\": func(input['sum'])\n",
    "#         }\n",
    "#     return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save cleaned dataset:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Save cleaned dataset: 100%|██████████| 4/4 [01:42<00:00, 25.51s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset_names)), desc=\"Save cleaned dataset\"):\n",
    "    dataset_name = dataset_names[i]\n",
    "    file_name = dataset_name + \".parquet\"\n",
    "    path = os.path.join(clean_dataset_path, file_name)\n",
    "    pq.write_table(table= datasets[dataset_name], where=path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
