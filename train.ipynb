{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref\n",
    "https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=SaPAR7TWmxoM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m63050123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nattanon\\Desktop\\Text-Summarization-model\\wandb\\run-20231006_132207-gy94rmg3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/63050123/Text-Summarization-model/runs/gy94rmg3' target=\"_blank\">rosy-firefly-20</a></strong> to <a href='https://wandb.ai/63050123/Text-Summarization-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/63050123/Text-Summarization-model' target=\"_blank\">https://wandb.ai/63050123/Text-Summarization-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/63050123/Text-Summarization-model/runs/gy94rmg3' target=\"_blank\">https://wandb.ai/63050123/Text-Summarization-model/runs/gy94rmg3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact slices:v2, 6632.04MB. 11 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   11 of 11 files downloaded.  \n",
      "Done. 0:0:8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\artifacts\\slices-v2\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('dylanon/text-summarization-model/slices:v2', type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "print(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>spanish property group colonial , struggling u...</td>\n",
       "      <td>spain  is colonial posts #.## billion euro loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>south korea  is inflation rate slowed slightly...</td>\n",
       "      <td>skorea  is inflation rate slows slightly in au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>south korean shares fell sharply late monday m...</td>\n",
       "      <td>skorean stocks sharply lower in morning session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>hundreds of australian holidaymakers stranded ...</td>\n",
       "      <td>charter flight to bring home australians stran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>malaysia  is financial markets are closed on m...</td>\n",
       "      <td>malaysia markets closed for holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916615</th>\n",
       "      <td>9149011</td>\n",
       "      <td>The song charted in June when Sheeran is secon...</td>\n",
       "      <td>Ed Sheeran is track Thinking Out Loud has topp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916616</th>\n",
       "      <td>9149017</td>\n",
       "      <td>Welsh ministers are hoping to use EU money to ...</td>\n",
       "      <td>The Welsh Government has been urged by a UK go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916617</th>\n",
       "      <td>9149019</td>\n",
       "      <td>The incident took place in Thorpe St Andrew, N...</td>\n",
       "      <td>A man has been arrested on suspicion of murder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916618</th>\n",
       "      <td>9149028</td>\n",
       "      <td>Alderney coastal defences were attacked by hug...</td>\n",
       "      <td>A severe storm with conditions described as \"h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916619</th>\n",
       "      <td>9149033</td>\n",
       "      <td>By Jane MowerBBC 2012 Watson, who suffered bra...</td>\n",
       "      <td>Former boxer Michael Watson and Paralympian Da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>916620 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text   \n",
       "0             7  spanish property group colonial , struggling u...  \\\n",
       "1            15  south korea  is inflation rate slowed slightly...   \n",
       "2            23  south korean shares fell sharply late monday m...   \n",
       "3            29  hundreds of australian holidaymakers stranded ...   \n",
       "4            34  malaysia  is financial markets are closed on m...   \n",
       "...         ...                                                ...   \n",
       "916615  9149011  The song charted in June when Sheeran is secon...   \n",
       "916616  9149017  Welsh ministers are hoping to use EU money to ...   \n",
       "916617  9149019  The incident took place in Thorpe St Andrew, N...   \n",
       "916618  9149028  Alderney coastal defences were attacked by hug...   \n",
       "916619  9149033  By Jane MowerBBC 2012 Watson, who suffered bra...   \n",
       "\n",
       "                                                      sum  \n",
       "0         spain  is colonial posts #.## billion euro loss  \n",
       "1       skorea  is inflation rate slows slightly in au...  \n",
       "2         skorean stocks sharply lower in morning session  \n",
       "3       charter flight to bring home australians stran...  \n",
       "4                     malaysia markets closed for holiday  \n",
       "...                                                   ...  \n",
       "916615  Ed Sheeran is track Thinking Out Loud has topp...  \n",
       "916616  The Welsh Government has been urged by a UK go...  \n",
       "916617  A man has been arrested on suspicion of murder...  \n",
       "916618  A severe storm with conditions described as \"h...  \n",
       "916619  Former boxer Michael Watson and Paralympian Da...  \n",
       "\n",
       "[916620 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show head of data in slice_i.parquet\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(artifact_dir + '/slice_8.parquet')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### example - use midel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   259,  87168,  69260,  58384, 140980,    259,  85461, 180411, 165600,\n",
      "         117548,  29373,  17636,  47414, 188445, 160082, 130621, 223122,    259,\n",
      "         182965,  11984,  23043,  17138, 130621, 223122,  18281,  76027,  53959,\n",
      "           2091,  85350,   9521,  35835,  53959,  11984,   1549,  23043, 168764,\n",
      "         222168, 228309,  90982, 146348,  76389,  67864,  95320, 170504,  17636,\n",
      "         210805,  82967,  89051, 222168,    259,  98179,   6196, 162969,   4682,\n",
      "          11984,  21746,  34904, 151489, 109663,  37087,  45204,   6196,  43427,\n",
      "          46865,  35835,  44954,  22087,  21942,   3682,  66806,  66936,  76389,\n",
      "          67864,  95320, 170504,  28032,    495,  84679,  51738,  45204,   6196,\n",
      "           1549,    356, 159695,  11984,  66936,    259,  76389,  67864,  95320,\n",
      "         170504,    670,  11817,   3324,  45204,   6196,   1549,    381, 159695,\n",
      "          11984,  66936,    670,   4935,  58094,  35835,  11984,    259,  66936,\n",
      "          76389,  67864,  95320, 170504,  79291, 147895,   6196,    259, 162017,\n",
      "         138467,   5269, 187323,  44573,  35835,  85350,  11984,  74302, 185473,\n",
      "          52596,    333,    259, 188537, 160082, 130621, 223122, 146348,  83627,\n",
      "          48786, 188537,  11858,    259, 186534,  11629, 166516,   6582,  36247,\n",
      "          11984, 210805, 186534,   5269,    381, 147895,  53868, 165852,  28032,\n",
      "            495,  28032,    495,   3324,    670,  18823,    259, 162017, 138467,\n",
      "           5269, 187323,  34447,  69260,  58384, 140980,  60099,   4552, 153880,\n",
      "          21005,    259,  85461, 180411, 165600, 117548,  29373, 116123,  71026,\n",
      "         176104, 122231,   1549, 152683,  21637,  39525,  93631,      1,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0]])\n",
      "ทีมชาติไทยคว้าแชมป์ตะกร้ออิสแทฟ ซูเปอร์ ซีรี่ส์ 2019 ที่ประเทศสิงคโปร์\n"
     ]
    }
   ],
   "source": [
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "\n",
    "article_text = \"\"\"การแข่งขันตะกร้อ ซูเปอร์ ซีรี่ส์ ที่ประเทศสิงคโปร์ รอบชิงชนะเลิศ ทีมชาติไทยผ่านเข้าชิงชนะเลิศทั้งประเภทชายและหญิง โดยทีมชายไทยที่ผ่านอินเดียมาได้ในรอบรองพบกับอินโดนีเซีย ที่เอาชนะเกาหลีใต้มาได้ แมตช์นี้ไทยยังรักษาสถิติไม่เคยเสียเซตแรกให้กับทีมใดก่อน เมื่อเป็นฝ่ายชนะอินโดนีเซีย 15-9 คะแนน ส่วนเซตที่ 2 ทีมไทยชนะ อินโดนีเซีย 15-8 และเซตที่ 3 ทีมไทยชนะ 15-3 ทำให้ทีมไทย ชนะอินโดนีเซีย 3-0 เซต คว้าแชมป์ไปครองส่วนทีมหญิงไทย ดีกรีอันดับ 1 ของโลก รอบชิงชนะเลิศพบกับอันดับที่สองของโลกอย่าง เวียดนาม ซึ่งปรากฏว่าสาวไทยเอาชนะเวียดนามไป 3 เซตรวด 15-9 15-9 และ 15-13 คว้าแชมป์ไปครอง สำหรับตะกร้ออิสแทฟ ซูเปอร์ ซีรี่ส์ รายการต่อไปจะมาแข่งขันที่กรุงเทพ ในเดือนมิถุนายน\"\"\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [WHITESPACE_HANDLER(article_text)],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")[\"input_ids\"]\n",
    "print(input_ids)\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=84,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=4\n",
    ")[0]\n",
    "\n",
    "\n",
    "summary = tokenizer.decode(\n",
    "    output_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "  # Convert the text and summary columns to tensors.\n",
    "  input_text = dataset[\"text\"]\n",
    "  target_text = dataset[\"sum\"]\n",
    "\n",
    "  # this line just follow model card in huggingface\n",
    "  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "  input_text = [WHITESPACE_HANDLER(text) for text in input_text]\n",
    "\n",
    "  # Tokenize the input and target text.\n",
    "  input_ids = tokenizer.batch_encode_plus( input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)[\"input_ids\"]\n",
    "  target_ids = tokenizer.batch_encode_plus( target_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=150)[\"input_ids\"]\n",
    "\n",
    "  return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"target_ids\": target_ids\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/10000 [04:48<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 20000/20000 [00:03<00:00, 5053.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for all slices.\n",
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the artifact.\n",
    "df = pd.read_parquet(artifact_dir + '/slice_0.parquet')[:20000]\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "# Preprocess the data.\n",
    "dataset = dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 16000/16000 [00:00<00:00, 69069.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 41959.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 45721.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# split data to train test and validation\n",
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1):\n",
    "  \"\"\"Split a dataset into train, validation, and test splits.\n",
    "\n",
    "  Args:\n",
    "    dataset: huggingface Dataset object.\n",
    "    train_ratio: The fraction of the data to use for training.\n",
    "    val_ratio: The fraction of the data to use for validation.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of three lists, the train, validation, and test splits.\n",
    "  \"\"\"\n",
    "  \n",
    "  # dataset = dataset.shuffle()\n",
    "  \n",
    "  split_dataset = dataset.train_test_split(train_size=train_ratio)\n",
    "  train_set = split_dataset[\"train\"]\n",
    "  data_subset = split_dataset[\"test\"]\n",
    "  split_dataset = data_subset.train_test_split(train_size=val_ratio / (1 - train_ratio))\n",
    "  val_set = split_dataset[\"train\"]\n",
    "  test_set = split_dataset[\"test\"]\n",
    "\n",
    "  return train_set, val_set, test_set\n",
    "\n",
    "# Split the processed dataset into train, validation, and test splits\n",
    "train_set, val_set, test_set = split_dataset(dataset)\n",
    "# sacve data to local\n",
    "train_set.save_to_disk(\"train_set\")\n",
    "val_set.save_to_disk(\"val_set\")\n",
    "test_set.save_to_disk(\"test_set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text', 'sum', 'input_ids', 'target_ids'],\n",
      "    num_rows: 16000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nattanon\\Desktop\\Text-Summarization-model\\train.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     run_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest-first-run\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mdataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nattanon/Desktop/Text-Summarization-model/train.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1898\u001b[0m ):\n\u001b[0;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\trainer.py:2776\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2773\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2775\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2776\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2779\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\trainer.py:2801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2799\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2800\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2802\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2803\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1771\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1768\u001b[0m         decoder_attention_mask \u001b[39m=\u001b[39m decoder_attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1770\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1771\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1772\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1773\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1774\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1775\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1776\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   1777\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1778\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1779\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1780\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1781\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1782\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1783\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1784\u001b[0m )\n\u001b[0;32m   1786\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1788\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\nattanon\\.platformio\\penv\\Lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:989\u001b[0m, in \u001b[0;36mMT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     err_msg_prefix \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minput_ids or \u001b[39m\u001b[39m{\u001b[39;00merr_msg_prefix\u001b[39m}\u001b[39;00m\u001b[39minputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    991\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    992\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# Define the training arguments with tensorflow.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1000,\n",
    "    \n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"test-first-run\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
